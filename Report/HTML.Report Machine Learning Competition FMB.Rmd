---
title: "Machine Learning Competition: Report on Forest Cover Type Prediction"
author: "Fernandez, Mari­a, McIver, Jordan, Besanson, Gaston"
output: html_document
---

```{r,echo=FALSE, message=FALSE}
setwd("~/Documents/Box Sync/Current/Machine Learning/MLcompetition")
source("Code/Graph.R")## the Graphs will be written in the Graph code, so here we only have to call them
```
# Introduction

\newthought{This report} has the purpose to explain our experience on the Forest Cover Type Prediction's Competition.^[https://inclass.kaggle.com/c/prediction-of-a-forest-covertype] This experience starts with `Data exploration and Feature Creation`, `Pre-Processing`, `Machine Learning Methods`, `Results over this dataset`, `Conclusion`.

Our main objetive for this project was to predicte as best as possible the type of tree in a region with the purpose of minimizing the fires. For this purpose we had to take into account two possible scenarios. The first would be, that all trees are equally important. So the main objective is to predict well the maximum trees possible. The second scenario would be the importance of misclassifying a concrete tree. For example, misclassifying very flamable tree would have much worse consecuences in preventing a fire. With this two escenarios we will try to predict the type of tree in our Data taking into consideration the different cost of missclassification.

# Data Description^[The Feature creation is explained in the attached powerpoint presentation.]

The data used in this report consists of 54 attributes and 50.000 observations for the training and 100.000 observations for the competition. The attributes with which we may classify the type of tree are 10 quantitative data, 4 binary wilderness areas and 40 binary soil type variables. The trees are classify in 7 different types.

One of the first things noticed in the data is that the portion of trees are very irregular. The following graph shows the portions of each type of tree in our data. We can see that mainly we have two type of trees *Spruce* and *Lodgepole Pine*, after those two types the other have a smaller presence. We will have to take this into account to check how our classifications methods work with the trees with less observations.

```{r,echo=FALSE}
plot1

```

First step before starting to "play" with the data has to scale the variables so they would all be *normalize* 

#are we sure?


The next thing we wanted to consider was creating some more informative variables in the data

##JORDAN YOU SHOULD EXPLAIN THIS!

**The Graph represent 3 variables that where already in the data set, and the second plot gives how our transformations might help classifying. I'm not sure they are helpful, will need to decide.**

```{r,echo=FALSE}


with(training_set,scatterplot3d(elevation, hor_dist_fire, hor_dist_road,
                                       color = colour, pch = 20,angle = 70, 
                                       col.grid = NULL, col.axis = "grey"))



with(training_set,scatterplot3d(fire_risk, horDistHyd_ElevShift, crowFliesHyd,
                                color = colour, pch = 20,angle = 90, 
                                col.grid = NULL, col.axis = "grey"))

```


# Pre-Processing



We wanted to try reducing dimensiality in order to decrees the run-time of our different classification models. However, the loss of variance would mean decreasing the models performance. We study the three main methods use in PCA with the linear, polynomial and radial transformation of the data. The following graph shows the portion of explained variance as we take more principal components. We can see that the only method we could consider would be the poplynomial transformation, since the other methods don't reduce well the dimensionality. 

```{r, fig.width = 7, fig.height = 3, fig.cap = "PCA"}
setwd("~/Documents/Box Sync/Current/Machine Learning/MLcompetition")

PCA<-read.table("Data/tables/PCA_Variance.csv",sep=";")

ggplot(PCA ,aes(x=PC,y=Variance,colour=Method))+geom_line()+theme_bw()+
  scale_x_continuous(breaks=seq(1,10,1))+ylab("Explained Variance")

```

Creating the vectors with the polynomial transformation may capture the variance, but will it help explain the Cover Type of our data? For a "sanety check" we test the new principal components vectors with the SVM method of classification. As we can see in the following graph to get a similar performance in the training error with the PCA we would need 30 components, and even then the classification is not stabel. So from this PCA study we decide to proceed without using PCA.

```{r, fig.width = 7, fig.height = 3, fig.cap = "PCA"}
setwd("~/Documents/Box Sync/Current/Machine Learning/MLcompetition")

PCA.error<-read.table("Data/tables/PCA_error.csv",sep=";")

ggplot(PCA.error,aes(x=PC,y=error,colour=type))+geom_line()+theme_bw()+
  scale_x_continuous(breaks=seq(0,30,5))

```



# Machine Learning Methods

From the theory classes and previous works in the same data base, we decided we where going to study the performance of Random Forest, Support Vector Machine, Gradient Boosting Methods and Deep Learning.

## Random Forest
What package we used. Why we select this method. Literature

## Support Vector Machine
Support Vector Machine is a algorithm that tries to find a hyperplane that splits the data with the maximum margin. When data is perfect separable we try to find a hard margin classifier that correctly classifies all the data points. When the data is not perfectly separable we introduce a slack variable and a cost related to the misclassification. In our case, the data is not perfectly separable so we introduce a cost for missclassification, further more, it can not be separated by a linear function. To solve this we introduce the kernel transformations and try classify the points. 

## Gradient Boosting Methods
What package we used. Why we select this method. Literature

## Deep Learning. Just for fun
What package we used. Why we select this method. Literature

# Results

Decide to do Cross Validation. The accuracy in the training. accuarcy in the leaderboard. ROC. Tables
IDEAS?

If we have the 3 methods.. we can do a voting......

# Conclusion


----------------------------------------------------------------

\begin{marginfigure}
$$\frac{d}{dx}\left( \int_{0}^{x} f(u)\,du\right)=f(x).$$
\caption{An equation}
\end{marginfigure}

Note the use of the `\caption` command to add additional text below the equation.

## Full Width Figures

You can arrange for figures to span across the entire page by using the `fig.fullwidth` chunk option. 

```{r, fig.width = 10, fig.height = 2, fig.fullwidth = TRUE, fig.cap = "Full width figure"}
library(ggplot2)
qplot(wt, mpg, data=mtcars, colour=factor(cyl))
```


# Tables

You can use the **xtable** package to format \LaTeX\ tables that integrate well with the rest of the Tufte handout style. Note that it's important to set the `xtable.comment` and `xtable.booktabs` options as shown below to ensure the table is formatted correctly for inclusion in the document.

```{r, results='asis'}
library(xtable)
options(xtable.comment = FALSE)
options(xtable.booktabs = TRUE)
xtable(head(mtcars[,1:6]), caption = "First rows of mtcars")
```