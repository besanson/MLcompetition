---
title: "Machine Learning Competition: Report on Forest Cover Type Prediction"
author: "Fernandez, Mari­a, McIver, Jordan, Besanson, Gaston"
output: html_document
---

# Introduction

\newthought{This report} has the purpose to explain our experience on the Forest Cover Type Prediction's Competition.^[https://inclass.kaggle.com/c/prediction-of-a-forest-covertype] This experience starts with `Data exploration and Feature Creation`, `Pre-Processing`, `Machine Learning Methods`, `Results over this dataset`, `Conclusion`.

# Data Description^[The Feature creation is explained in the attached powerpoint presentation.]

how many features. how many classes. Table explaning what is the class more populated. In test and trainning set.
IDEAS?

# Pre-Processing

We wanted to try reducing dimensiality in order to decrees the run-time of our different classification models. However, the loos of variance would mean decreasing the models performance. We study the three main methods use in PCA with the linear, polynomial and radial transformation of the data. The following graph shows the portion of explained variance as we take more principal components. We can see that the only method we could consider would be the poplynomial transformation, since the other methods don't reduce well the dimensionality. 

```{r, fig.width = 7, fig.height = 3, fig.cap = "PCA"}
library(ggplot2)
setwd("~/Documents/Box Sync/Current/Machine Learning/MLcompetition")
PCA<-read.table("Data/tables/PCA_Variance.csv",sep=";")

ggplot(PCA ,aes(x=PC,y=Variance,colour=Method))+geom_line()+theme_bw()+
  scale_x_continuous(breaks=seq(1,10,1))+ylab("Explained Variance")

```

Creating the vectors with the polynomial transformation may capture the variance, but will it help explain the Cover Type of our data? For a "sanety check" we test the new principal components vectors with the SVM method of classification. As we can see in the following graph to get a similar performance in the training error with the PCA we would need 30 components, and even then the classification is not stabel. So from this PCA study we decide to proceed without using PCA.

```{r, fig.width = 7, fig.height = 3, fig.cap = "PCA"}
library(ggplot2)
setwd("~/Documents/Box Sync/Current/Machine Learning/MLcompetition")
PCA.error<-read.table("Data/tables/PCA_error.csv",sep=";")

ggplot(PCA.error,aes(x=PC,y=error,colour=type))+geom_line()+theme_bw()+
  scale_x_continuous(breaks=seq(0,30,5))

```



# Machine Learning Methods

## Random Forest
What package we used. Why we select this method. Literature

## Support Vector Machine
What package we used. Why we select this method. Literature

## Gradient Boosting Methods
What package we used. Why we select this method. Literature

## Deep Learning. Just for fun
What package we used. Why we select this method. Literature

# Results

Decide to do Cross Validation. The accuracy in the training. accuarcy in the leaderboard. ROC. Tables
IDEAS?

If we have the 3 methods.. we can do a voting......

# Conclusion


----------------------------------------------------------------

\begin{marginfigure}
$$\frac{d}{dx}\left( \int_{0}^{x} f(u)\,du\right)=f(x).$$
\caption{An equation}
\end{marginfigure}

Note the use of the `\caption` command to add additional text below the equation.

## Full Width Figures

You can arrange for figures to span across the entire page by using the `fig.fullwidth` chunk option. 

```{r, fig.width = 10, fig.height = 2, fig.fullwidth = TRUE, fig.cap = "Full width figure"}
library(ggplot2)
qplot(wt, mpg, data=mtcars, colour=factor(cyl))
```


# Tables

You can use the **xtable** package to format \LaTeX\ tables that integrate well with the rest of the Tufte handout style. Note that it's important to set the `xtable.comment` and `xtable.booktabs` options as shown below to ensure the table is formatted correctly for inclusion in the document.

```{r, results='asis'}
library(xtable)
options(xtable.comment = FALSE)
options(xtable.booktabs = TRUE)
xtable(head(mtcars[,1:6]), caption = "First rows of mtcars")
```